---
title: "Test"
author: "Austin Hill"
date: "August 14, 2018"
output:
  word_document: default
  pdf_document: default
---
#Prepping data
```{r setup, message = F}
library(dplyr)
library(ggplot2)
library(corrplot)
library(caret)
library(MASS)
library(randomForest)
dat <- as.data.frame(read.csv("C:/Users/mqgy9210/Downloads/Data_Analyst_Take_Home_File.csv", header = T))
```

#Check for missing data and make sure data types are appropriate
```{r}
sapply(dat, function(x) sum(is.na(x))) 
#No missing data
str(dat)
#Fix data types
dat$Night.Calls <- as.numeric(dat$Night.Calls)
dat$Area.Code <- as.factor(dat$Area.Code)
dat$Churn <- as.factor(dat$Churn)
table(dat$Churn) 
#A little unbalanced but 483 is a sufficient positive sample
table(dat$State) 
#ARZ is a typo so we change it to AZ
dat[dat$State == "ARZ", "State"] <- "AZ"
#Check out summary statistics to see if anything is glaringly wrong
summary(dat)
```

#Checking correlation to find variables that can be excluded
```{r}
dat.num <- subset(dat, select=-c(Churn, Int.l.Plan, VMail.Plan, State, Area.Code, Phone))
dat.num <- sapply(dat.num, function(x) as.numeric(x))
corr.matrix <- cor(dat.num)
corrplot(corr.matrix, main="\n\nCorrelation Plot") 
#Mins highly correlated with charges, as expected, so we remove charges
dat2 <- subset(dat, select = -c(Day.Charge, Eve.Charge, Night.Charge, Intl.Charge))

```

#Checking distributions of categorical variables.
```{r}
ggplot(dat2, aes(x = State)) + geom_histogram(stat = "count") + xlab("State")
ggplot(dat2, aes(x = factor(dat$VMail.Plan))) + geom_histogram(stat = "count") + xlab("Vmail")
ggplot(dat2, aes(x = factor(dat$Int.l.Plan))) + geom_histogram(stat = "count") + xlab("Int")
ggplot(dat2, aes(x = factor(dat$Area.Code))) + geom_histogram(stat = "count") + xlab("Area Code")
#International plan is unbalanced but this is not an issue because our sample is large enough 
#Phone is by individual and therefore perfectly predictive so we remove it
dat3 <- subset(dat2, select=-c(Phone))
```

#Logistic regression classification model
```{r}
#Create a training and a test set
t<- createDataPartition(dat$Churn,p=0.7,list=FALSE)
set.seed(1111)
train<- dat3[t,]
test<- dat3[-t,]
logistic <- glm(Churn ~ ., family=binomial(link="logit"), data=train)
summary(logistic) 
#State is not significantly related to churn
#International plan and customer service calls have high estimates and therefore a large impact on churn
anova(logistic, test = "Chisq") 
#Day minutes, customer service calls, and international plan all substantially reduce residual deviance and are important predictors

```

#Assessing predictive value of logistic model
```{r}
#Fitting predicted values from the model
fitted <- predict(logistic, test, type='response')
#Classifying as churn or not using a cutoff probability of 0.5
fitted <- ifelse(fitted > 0.5,1,0)
error <- mean(fitted != test$Churn)
print(paste('Logistic Regression Accuracy',1-error))
#The accuracy is quite good
```

#Based on the model above, State is not very significant so we'll try removing it
```{r}
dat4 <- subset(dat3, select = -c(State))
train<- dat4[t,]
test<- dat4[-t,]
logistic <- glm(Churn ~ ., family=binomial(link="logit"), data=train)
fitted <- predict(logistic, test, type='response')
fitted <- ifelse(fitted > 0.5,1,0)
error <- mean(fitted != test$Churn)
print(paste('Logistic regression accuracy:',1-error)) 
#Accuracy is significantly improved by removing state
```

#Random forest model to compare to logistic model
```{r}
rf=randomForest(Churn ~ . , data = train , importance = T, ntrees = 1000)
varImpPlot(rf) 
#We can see that day minutes, international plan, customer service calls, and evening minutes are most predictive of churn, this is in agreement with the logistic model

prediction <- predict(rf, test, type = 'response')
error <- mean(prediction != test$Churn)
print(paste('Random forest accuracy:',1-error))  
#The prediction accuracy is very good and a big improvement over the logistic model
```

#In Summary
Based on these results, this random forest model can be used to accurately predict whether a customer will churn based on a variety of metrics, most important being the number of minutes a customer talks per day, whether they have an international plan, the number of customer service calls they make and the number of minutes they talk per evening. If I were to implement a strategy to retain customers at risk of churn, I would recommend focusing efforts on reducing number of customer service calls and ensuring that the international plan is working well for customers. These metrics had the largest odds ratios for churn and will therefore have the largest immediate impact.
